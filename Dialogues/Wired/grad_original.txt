<b>B:</b>I'm in my first year of a PhD in Computer Science and I'm studying natural language processing and machine learning.<br>
<b>A:</b>So would you mind telling me a bit about what you've been working on or interested in lately?<br>
<b>B:</b>I've been looking at understanding persuasion in online text and the ways that we might be able to automatically detect the intent behind that persuasion or who it's targeted at and what makes effective persuasive techniques.<br>
<b>A:</b>So what are some of the techniques you're applying to look at that debate data?<br>
<b>B:</b>Something I'm interested in exploring is how well it works to use deep learning and sort of automatically extracted features from this text versus using some of the more traditional techniques that we have, things like lexicons or some sort of template matching techniques for extracting features from texts. That's a question I'm just interested in, in general. When do we really need deep learning versus when can we use something that's a little bit more interpretable, something that's been around for a while?<br>
<b>A:</b>Do you think there are going to be general principles that guide those decisions? Because right now, it's generally up to the machine learning engineer to decide what tools they want to apply.<br>
<b>B:</b>I definitely think there is, but I also, sort of, see it varying a lot based on the use case, something that, kind of, works out of the box and maybe works a little bit more automatically might be better. And in other cases, you do, sort of, kind of, you want a lot of fine grain control.<br>
<b>A:</b>So is that where some of that frustration around the lack of controllability and interpretability comes from?<br>
<b>B:</b>Yeah, if you're building a model that just predicts the next thing based off of everything it's seen from texts online, then yeah, you're really gonna be replicating whatever that distribution online is.<br>
<b>A:</b>If you train a model off of language off the internet, it sometimes says uncomfortable things or inappropriate things and sometimes really biased things. Have you ever run into this yourself? And then how do you think about that problem of potentially even measuring the bias in a model that we've trained?<br>
<b>B:</b>Yeah, it's a really tricky question. As you said, these models are trained to, sort of, predict the next sequence of words, given a certain sequence of words. So we could start with just, sort of, prompts like the woman was versus the man was, and, kind of, pull out common words that are, sort of, more used with one phrase versus the other. So that's, sort of, a qualitative way of looking at it. It's not ever kind of a guarantee of how the model is gonna behave in one particular instance. And I think that's what's really tricky and that's why I, sort of, think it's really good for creators of systems to just be honest about, This is, sort of, what we have seen. And so then, someone can make their own judgment about, Is this gonna be too high risk for, sort of, my particular use case?<br>
<b>A:</b>I imagine in the last few years, we've seen a lot of changes and improvements in the capabilities of NLP systems. So is there anything in that that you're particularly excited about exploring further?<br>
<b>B:</b>I'm really interested in, sort of, the creative potential that we've started to see from NLP systems with things like GPT-3 and other really powerful language models. It's really easy to write long grammatical passages thinking about the way that we can then harness, like, the human ability to actually give meaning to those words and, sort of, provide structure and how we can combine those things with the, kind of like, generative capabilities of these models now is really interesting.<br>
<b>A:</b>Yeah, I agree.